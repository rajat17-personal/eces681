{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc6572d6",
   "metadata": {},
   "source": [
    "# CIFAR-10 Classification with Hyperparameter Tuning using Ray Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e554f6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune import Tuner, TuneConfig\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.air import session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df078c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ebb4b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465),(0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "train_val_set = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_set = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "num_train = 49000\n",
    "num_val = 1000\n",
    "train_set, val_set = torch.utils.data.random_split(train_val_set, [num_train, len(train_val_set)-num_train])\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=128, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_set, batch_size=128, shuffle=False, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_set, batch_size=128, shuffle=False, num_workers=4, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a74d321b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db2836c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fc_model(config, train_loader=None, val_loader=None):\n",
    "    model = TwoLayerNet(3 * 32 * 32, config[\"hidden_dim\"], 10).to(device)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=config[\"lr\"])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def l2_regularization(model):\n",
    "        return sum(torch.norm(p, 2)**2 for name, p in model.named_parameters() if 'weight' in name)\n",
    "\n",
    "    for _ in range(config[\"epochs\"]):\n",
    "        model.train()\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss += config[\"reg\"] * l2_regularization(model)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    correct = total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            correct += (outputs.argmax(1) == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    session.report({\"val_accuracy\": correct / total})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67692fdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2025-08-07 15:11:43</td></tr>\n",
       "<tr><td>Running for: </td><td>00:00:07.62        </td></tr>\n",
       "<tr><td>Memory:      </td><td>15.3/15.5 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=0<br>Bracket: Iter 64.000: None | Iter 16.000: None | Iter 4.000: None | Iter 1.000: None<br>Logical resource usage: 10.0/28 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "<div class=\"messages\">\n",
       "  <h3>Messages</h3>\n",
       "  : ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
       "  \n",
       "  \n",
       "</div>\n",
       "<style>\n",
       ".messages {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  padding-left: 1em;\n",
       "  overflow-y: auto;\n",
       "}\n",
       ".messages h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n",
       "\n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc                  </th><th style=\"text-align: right;\">  hidden_dim</th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  reg</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_fc_model_56737_00000</td><td>RUNNING </td><td>172.27.229.196:193115</td><td style=\"text-align: right;\">        1000</td><td style=\"text-align: right;\">0.1   </td><td style=\"text-align: right;\">0.05 </td></tr>\n",
       "<tr><td>train_fc_model_56737_00001</td><td>RUNNING </td><td>172.27.229.196:193116</td><td style=\"text-align: right;\">         256</td><td style=\"text-align: right;\">0.1   </td><td style=\"text-align: right;\">0.005</td></tr>\n",
       "<tr><td>train_fc_model_56737_00002</td><td>PENDING </td><td>                     </td><td style=\"text-align: right;\">         256</td><td style=\"text-align: right;\">0.1   </td><td style=\"text-align: right;\">0.005</td></tr>\n",
       "<tr><td>train_fc_model_56737_00003</td><td>PENDING </td><td>                     </td><td style=\"text-align: right;\">         512</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">0.005</td></tr>\n",
       "<tr><td>train_fc_model_56737_00004</td><td>PENDING </td><td>                     </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">0.1   </td><td style=\"text-align: right;\">0.01 </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught unexpected exception: Task was killed due to the node running low on memory.\nMemory on the node (IP: 172.27.229.196, ID: 4c474c019b2ca202b6fd6001b8f3c655d11f437df1a62ba5dfd9093d) where the task (actor ID: 031542740f78cdba16d1ffe901000000, name=ImplicitFunc.__init__, pid=193116, memory used=0.46GB) was running was 15.48GB / 15.53GB (0.996813), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 9d7f413fa7f20a8b5720c5819209dec99de3169a43b31cfd50c44ff7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.229.196`. To see the logs of the worker, use `ray logs worker-9d7f413fa7f20a8b5720c5819209dec99de3169a43b31cfd50c44ff7*out -ip 172.27.229.196. Top 10 memory users:\nPID\tMEM(GB)\tCOMMAND\n181774\t1.11\t/home/sraja/.vscode-server/bin/488a1f239235055e34e673291fb8d8c810886f81/node /home/sraja/.vscode-ser...\n185449\t1.01\t/home/sraja/miniconda3/envs/pytorch/bin/python -m ipykernel_launcher --f=/run/user/1000/jupyter/runt...\n44572\t0.97\t/home/sraja/miniconda3/envs/pytorch/bin/python -m ipykernel_launcher --f=/run/user/1000/jupyter/runt...\n191085\t0.79\t/home/sraja/miniconda3/envs/pytorch/bin/python -m ipykernel_launcher --f=/run/user/1000/jupyter/runt...\n185358\t0.72\t/home/sraja/miniconda3/envs/pytorch/bin/python -m ipykernel_launcher --f=/run/user/1000/jupyter/runt...\n191017\t0.71\t/home/sraja/miniconda3/envs/pytorch/bin/python -m ipykernel_launcher --f=/run/user/1000/jupyter/runt...\n488\t0.63\t/home/sraja/.vscode-server/bin/488a1f239235055e34e673291fb8d8c810886f81/node --dns-result-order=ipv4...\n44291\t0.50\t/home/sraja/miniconda3/envs/pytorch/bin/python -m ipykernel_launcher --f=/run/user/1000/jupyter/runt...\n48433\t0.48\t/home/sraja/miniconda3/envs/pytorch/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n193116\t0.46\t\nRefer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pytorch/lib/python3.12/site-packages/ray/air/execution/_internal/event_manager.py:110\u001b[39m, in \u001b[36mRayEventManager.resolve_future\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m     result = \u001b[43mray\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfuture\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pytorch/lib/python3.12/site-packages/ray/_private/auto_init_hook.py:22\u001b[39m, in \u001b[36mwrap_auto_init.<locals>.auto_init_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     21\u001b[39m auto_init_ray()\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pytorch/lib/python3.12/site-packages/ray/_private/client_mode_hook.py:104\u001b[39m, in \u001b[36mclient_mode_hook.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    103\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(ray, func.\u001b[34m__name__\u001b[39m)(*args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pytorch/lib/python3.12/site-packages/ray/_private/worker.py:2858\u001b[39m, in \u001b[36mget\u001b[39m\u001b[34m(object_refs, timeout)\u001b[39m\n\u001b[32m   2857\u001b[39m \u001b[38;5;66;03m# TODO(ujvl): Consider how to allow user to retrieve the ready objects.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2858\u001b[39m values, debugger_breakpoint = \u001b[43mworker\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_objects\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobject_refs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2859\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(values):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pytorch/lib/python3.12/site-packages/ray/_private/worker.py:960\u001b[39m, in \u001b[36mWorker.get_objects\u001b[39m\u001b[34m(self, object_refs, timeout, return_exceptions, skip_deserialization)\u001b[39m\n\u001b[32m    959\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m960\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m value\n\u001b[32m    962\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m values, debugger_breakpoint\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: Task was killed due to the node running low on memory.\nMemory on the node (IP: 172.27.229.196, ID: 4c474c019b2ca202b6fd6001b8f3c655d11f437df1a62ba5dfd9093d) where the task (actor ID: 031542740f78cdba16d1ffe901000000, name=ImplicitFunc.__init__, pid=193116, memory used=0.46GB) was running was 15.48GB / 15.53GB (0.996813), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 9d7f413fa7f20a8b5720c5819209dec99de3169a43b31cfd50c44ff7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.229.196`. To see the logs of the worker, use `ray logs worker-9d7f413fa7f20a8b5720c5819209dec99de3169a43b31cfd50c44ff7*out -ip 172.27.229.196. Top 10 memory users:\nPID\tMEM(GB)\tCOMMAND\n181774\t1.11\t/home/sraja/.vscode-server/bin/488a1f239235055e34e673291fb8d8c810886f81/node /home/sraja/.vscode-ser...\n185449\t1.01\t/home/sraja/miniconda3/envs/pytorch/bin/python -m ipykernel_launcher --f=/run/user/1000/jupyter/runt...\n44572\t0.97\t/home/sraja/miniconda3/envs/pytorch/bin/python -m ipykernel_launcher --f=/run/user/1000/jupyter/runt...\n191085\t0.79\t/home/sraja/miniconda3/envs/pytorch/bin/python -m ipykernel_launcher --f=/run/user/1000/jupyter/runt...\n185358\t0.72\t/home/sraja/miniconda3/envs/pytorch/bin/python -m ipykernel_launcher --f=/run/user/1000/jupyter/runt...\n191017\t0.71\t/home/sraja/miniconda3/envs/pytorch/bin/python -m ipykernel_launcher --f=/run/user/1000/jupyter/runt...\n488\t0.63\t/home/sraja/.vscode-server/bin/488a1f239235055e34e673291fb8d8c810886f81/node --dns-result-order=ipv4...\n44291\t0.50\t/home/sraja/miniconda3/envs/pytorch/bin/python -m ipykernel_launcher --f=/run/user/1000/jupyter/runt...\n48433\t0.48\t/home/sraja/miniconda3/envs/pytorch/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n193116\t0.46\t\nRefer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m      1\u001b[39m tuner_fc = tune.Tuner(\n\u001b[32m      2\u001b[39m     tune.with_resources(\n\u001b[32m      3\u001b[39m         tune.with_parameters(train_fc_model, train_loader=train_loader, val_loader=val_loader),\n\u001b[32m   (...)\u001b[39m\u001b[32m     18\u001b[39m     )\n\u001b[32m     19\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m results_fc = \u001b[43mtuner_fc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m best_fc = results_fc.get_best_result(metric=\u001b[33m\"\u001b[39m\u001b[33mval_accuracy\u001b[39m\u001b[33m\"\u001b[39m, mode=\u001b[33m\"\u001b[39m\u001b[33mmax\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mBest FC config:\u001b[39m\u001b[33m\"\u001b[39m, best_fc.config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pytorch/lib/python3.12/site-packages/ray/tune/tuner.py:345\u001b[39m, in \u001b[36mTuner.fit\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    313\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Executes hyperparameter tuning job as configured and returns result.\u001b[39;00m\n\u001b[32m    314\u001b[39m \n\u001b[32m    315\u001b[39m \u001b[33;03mFailure handling:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    341\u001b[39m \u001b[33;03m    RayTaskError: If user-provided trainable raises an exception\u001b[39;00m\n\u001b[32m    342\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    344\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._is_ray_client:\n\u001b[32m--> \u001b[39m\u001b[32m345\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_local_tuner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    346\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    347\u001b[39m     (\n\u001b[32m    348\u001b[39m         progress_reporter,\n\u001b[32m    349\u001b[39m         string_queue,\n\u001b[32m    350\u001b[39m     ) = \u001b[38;5;28mself\u001b[39m._prepare_remote_tuner_for_jupyter_progress_reporting()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pytorch/lib/python3.12/site-packages/ray/tune/impl/tuner_internal.py:506\u001b[39m, in \u001b[36mTunerInternal.fit\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    504\u001b[39m param_space = copy.deepcopy(\u001b[38;5;28mself\u001b[39m.param_space)\n\u001b[32m    505\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._is_restored:\n\u001b[32m--> \u001b[39m\u001b[32m506\u001b[39m     analysis = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_space\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    508\u001b[39m     analysis = \u001b[38;5;28mself\u001b[39m._fit_resume(trainable, param_space)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pytorch/lib/python3.12/site-packages/ray/tune/impl/tuner_internal.py:622\u001b[39m, in \u001b[36mTunerInternal._fit_internal\u001b[39m\u001b[34m(self, trainable, param_space)\u001b[39m\n\u001b[32m    609\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Fitting for a fresh Tuner.\"\"\"\u001b[39;00m\n\u001b[32m    610\u001b[39m args = {\n\u001b[32m    611\u001b[39m     **\u001b[38;5;28mself\u001b[39m._get_tune_run_arguments(trainable),\n\u001b[32m    612\u001b[39m     **\u001b[38;5;28mdict\u001b[39m(\n\u001b[32m   (...)\u001b[39m\u001b[32m    620\u001b[39m     **\u001b[38;5;28mself\u001b[39m._tuner_kwargs,\n\u001b[32m    621\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m622\u001b[39m analysis = \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    623\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    624\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    625\u001b[39m \u001b[38;5;28mself\u001b[39m.clear_remote_string_queue()\n\u001b[32m    626\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m analysis\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pytorch/lib/python3.12/site-packages/ray/tune/tune.py:994\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, storage_path, storage_filesystem, search_alg, scheduler, checkpoint_config, verbose, progress_reporter, log_to_file, trial_name_creator, trial_dirname_creator, sync_config, export_formats, max_failures, fail_fast, restore, resume, resume_config, reuse_actors, raise_on_failed_trial, callbacks, max_concurrent_trials, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, chdir_to_trial_dir, local_dir, _remote, _remote_string_queue, _entrypoint)\u001b[39m\n\u001b[32m    992\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    993\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m runner.is_finished() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m experiment_interrupted_event.is_set():\n\u001b[32m--> \u001b[39m\u001b[32m994\u001b[39m         \u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    995\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m has_verbosity(Verbosity.V1_EXPERIMENT):\n\u001b[32m    996\u001b[39m             _report_progress(runner, progress_reporter)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pytorch/lib/python3.12/site-packages/ray/tune/execution/tune_controller.py:685\u001b[39m, in \u001b[36mTuneController.step\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    682\u001b[39m \u001b[38;5;28mself\u001b[39m._maybe_add_actors()\n\u001b[32m    684\u001b[39m \u001b[38;5;66;03m# Handle one event\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m685\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_actor_manager\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[32m    686\u001b[39m     \u001b[38;5;66;03m# If there are no actors running, warn about potentially\u001b[39;00m\n\u001b[32m    687\u001b[39m     \u001b[38;5;66;03m# insufficient resources\u001b[39;00m\n\u001b[32m    688\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._actor_manager.num_live_actors:\n\u001b[32m    689\u001b[39m         \u001b[38;5;28mself\u001b[39m._insufficient_resources_manager.on_no_available_trials(\n\u001b[32m    690\u001b[39m             \u001b[38;5;28mself\u001b[39m.get_trials()\n\u001b[32m    691\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pytorch/lib/python3.12/site-packages/ray/air/execution/_internal/actor_manager.py:223\u001b[39m, in \u001b[36mRayActorManager.next\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    221\u001b[39m     \u001b[38;5;28mself\u001b[39m._actor_state_events.resolve_future(future)\n\u001b[32m    222\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m future \u001b[38;5;129;01min\u001b[39;00m actor_task_futures:\n\u001b[32m--> \u001b[39m\u001b[32m223\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_actor_task_events\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresolve_future\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfuture\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    224\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    225\u001b[39m     \u001b[38;5;28mself\u001b[39m._handle_ready_resource_future()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pytorch/lib/python3.12/site-packages/ray/air/execution/_internal/event_manager.py:113\u001b[39m, in \u001b[36mRayEventManager.resolve_future\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    112\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m on_error:\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m         \u001b[43mon_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    115\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pytorch/lib/python3.12/site-packages/ray/air/execution/_internal/actor_manager.py:771\u001b[39m, in \u001b[36mRayActorManager._schedule_tracked_actor_task.<locals>.on_error\u001b[39m\u001b[34m(exception)\u001b[39m\n\u001b[32m    770\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mon_error\u001b[39m(exception: \u001b[38;5;167;01mException\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m771\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_actor_task_failed\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    772\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtracked_actor_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtracked_actor_task\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexception\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexception\u001b[49m\n\u001b[32m    773\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pytorch/lib/python3.12/site-packages/ray/air/execution/_internal/actor_manager.py:290\u001b[39m, in \u001b[36mRayActorManager._actor_task_failed\u001b[39m\u001b[34m(self, tracked_actor_task, exception)\u001b[39m\n\u001b[32m    288\u001b[39m         tracked_actor_task._on_error(tracked_actor, exception)\n\u001b[32m    289\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m290\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    291\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCaught unexpected exception: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexception\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    292\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexception\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: Caught unexpected exception: Task was killed due to the node running low on memory.\nMemory on the node (IP: 172.27.229.196, ID: 4c474c019b2ca202b6fd6001b8f3c655d11f437df1a62ba5dfd9093d) where the task (actor ID: 031542740f78cdba16d1ffe901000000, name=ImplicitFunc.__init__, pid=193116, memory used=0.46GB) was running was 15.48GB / 15.53GB (0.996813), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 9d7f413fa7f20a8b5720c5819209dec99de3169a43b31cfd50c44ff7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.229.196`. To see the logs of the worker, use `ray logs worker-9d7f413fa7f20a8b5720c5819209dec99de3169a43b31cfd50c44ff7*out -ip 172.27.229.196. Top 10 memory users:\nPID\tMEM(GB)\tCOMMAND\n181774\t1.11\t/home/sraja/.vscode-server/bin/488a1f239235055e34e673291fb8d8c810886f81/node /home/sraja/.vscode-ser...\n185449\t1.01\t/home/sraja/miniconda3/envs/pytorch/bin/python -m ipykernel_launcher --f=/run/user/1000/jupyter/runt...\n44572\t0.97\t/home/sraja/miniconda3/envs/pytorch/bin/python -m ipykernel_launcher --f=/run/user/1000/jupyter/runt...\n191085\t0.79\t/home/sraja/miniconda3/envs/pytorch/bin/python -m ipykernel_launcher --f=/run/user/1000/jupyter/runt...\n185358\t0.72\t/home/sraja/miniconda3/envs/pytorch/bin/python -m ipykernel_launcher --f=/run/user/1000/jupyter/runt...\n191017\t0.71\t/home/sraja/miniconda3/envs/pytorch/bin/python -m ipykernel_launcher --f=/run/user/1000/jupyter/runt...\n488\t0.63\t/home/sraja/.vscode-server/bin/488a1f239235055e34e673291fb8d8c810886f81/node --dns-result-order=ipv4...\n44291\t0.50\t/home/sraja/miniconda3/envs/pytorch/bin/python -m ipykernel_launcher --f=/run/user/1000/jupyter/runt...\n48433\t0.48\t/home/sraja/miniconda3/envs/pytorch/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --l...\n193116\t0.46\t\nRefer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m [2025-08-07 15:12:34,109 E 191306 191306] (raylet) node_manager.cc:3041: 5 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 4c474c019b2ca202b6fd6001b8f3c655d11f437df1a62ba5dfd9093d, IP: 172.27.229.196) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.27.229.196`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    }
   ],
   "source": [
    "tuner_fc = tune.Tuner(\n",
    "    tune.with_resources(\n",
    "        tune.with_parameters(train_fc_model, train_loader=train_loader, val_loader=val_loader),\n",
    "        resources={\"cpu\": 5, \"gpu\": 0.5}\n",
    "    ),\n",
    "    param_space={\n",
    "        \"lr\": tune.choice([0.01, 0.05, 0.1, 0.001, 0.0005]),\n",
    "        \"hidden_dim\": tune.choice([128, 256, 512, 1000, 2000]),\n",
    "        \"reg\": tune.choice([0.0001, 0.001, 0.01, 0.005, 0.0005, 0.05]),\n",
    "        \"epochs\": 20\n",
    "    },\n",
    "    tune_config=tune.TuneConfig(\n",
    "        num_samples=50,\n",
    "        metric=\"val_accuracy\",\n",
    "        mode=\"max\",\n",
    "        scheduler=ASHAScheduler(),\n",
    "        max_concurrent_trials=5\n",
    "    )\n",
    ")\n",
    "\n",
    "results_fc = tuner_fc.fit()\n",
    "best_fc = results_fc.get_best_result(metric=\"val_accuracy\", mode=\"max\")\n",
    "print(\"Best FC config:\", best_fc.config)\n",
    "print(\"Best FC accuracy:\", best_fc.metrics[\"val_accuracy\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
